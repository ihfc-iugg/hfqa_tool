{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be211528-c9d1-424c-9ee8-09840a959842",
   "metadata": {},
   "source": [
    "    Author: Saman Firdaus Chishti   |   chishti@gfz-potsdam.de\n",
    "\n",
    "    Start date: 29-02-2024\n",
    "\n",
    "    \n",
    "**Description:** This set of code has been developed to check whether all the values entered in a Heatflow database adhere to a controlled vocabulary and proper structure. It generates an error message for each entry where the value entered is out of bounds and does not meet the assigned criteria. The code also enables checking the vocabulary for multiple values entered in a single column for a particular Heatflow data entry.\n",
    "This is in compliance with the paper by Fuchs et al. (2023) titled \"[Quality-assurance of heat-flow data: The new structure and evaluation scheme of the IHFC Global Heat Flow Database](https://doi.org/10.1016/j.tecto.2023.229976),\" published in Tectonophysics 863: 229976. Also revised for the newer release 2024.\n",
    "\n",
    "The code is intended to be published  for the global scientific community to check the quality of any Heatflow dataset adhering to the data structure described in the aforementioned scientific paper. It's a recommended prerequisite before calculating 'Quality Scores' for a given Heatflow dataset. The code for calculating 'Quality Scores' is provided in a separate document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05274602",
   "metadata": {},
   "source": [
    "![Vocab Image](Graphics/Vocab.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae9857-5d00-4f40-8ef9-21522f20e6ee",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f086e405-89c5-428e-a43c-40dc13c9aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import re\n",
    "from hfqa_tool.utils.utils import (\n",
    "    readable,\n",
    "    remove_head,\n",
    "    assign_columns,\n",
    "    assign_values,\n",
    "    safe_float_conversion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc73f416-d561-4427-b644-f320b289cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67cbfed-94ad-4703-a66b-3bd4bd72dba3",
   "metadata": {},
   "source": [
    "# 2. Controlled vocabulary\n",
    "## 2.1. Assigning columns with similar data types to specific list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "921e77c8-ca29-4131-9564-e6d0cfb694e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NumC, StrC, DateC = assign_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de2f4a-2a44-4d2b-8276-287662725379",
   "metadata": {},
   "source": [
    "## 2.2. Numeric value sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6cb62",
   "metadata": {},
   "source": [
    "    [Description]: Assign permissible value ranges for columns that store numeric values. The Allowed range of values are taken from \"Appendix A. Structure and field definitions of the IHFC Global Heat Flow Database\" in the aforementioned paper. Also revised for the newer release 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a38c444-1f07-4398-8ec6-29c38afeafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = {\n",
    "    #'ID': ['P1','P2','P4','P5','P6','P10','P11','C1','C2','C4','C5','C6','C22','C23','C24','C27','C28','C29','C30','C33','C34','C37','C39','C40','C47'],\n",
    "    'Min': [-999999.9,0,-90.00000,-180.00000,-12000,-12000,-12000,-999999.9,0,0,0,0,0,-9.99,-99999.99,-99999.99,-99999.99,-99999.99,0,0,0,0,0,0],\n",
    "    'Max': [999999.9,999999.9,90.00000,180.00000,9000,9000,9000,999999.9,19999.9,19999.9,999.9,99.99,99.99,99.99,99999.99,99999.99,99999.99,99999.99,99999,99999,999999,99.99,99.99,9999]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5ab7eaa-62ce-4f87-85bb-d5a62a569a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = pd.DataFrame(num_data, index=NumC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e3957-6fa9-4ded-9d54-f2b9669c55d3",
   "metadata": {},
   "source": [
    "### 2.2.1. Pivot the DataFrame: Rows become columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7b34661-9627-4f6e-93cd-3c5be45a8116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P10</th>\n",
       "      <th>P11</th>\n",
       "      <th>C1</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>...</th>\n",
       "      <th>C27</th>\n",
       "      <th>C28</th>\n",
       "      <th>C29</th>\n",
       "      <th>C30</th>\n",
       "      <th>C33</th>\n",
       "      <th>C34</th>\n",
       "      <th>C37</th>\n",
       "      <th>C39</th>\n",
       "      <th>C40</th>\n",
       "      <th>C47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>-999999.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-12000.0</td>\n",
       "      <td>-12000.0</td>\n",
       "      <td>-12000.0</td>\n",
       "      <td>-999999.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-99999.99</td>\n",
       "      <td>-99999.99</td>\n",
       "      <td>-99999.99</td>\n",
       "      <td>-99999.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>999999.9</td>\n",
       "      <td>999999.9</td>\n",
       "      <td>90.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>999999.9</td>\n",
       "      <td>19999.9</td>\n",
       "      <td>19999.9</td>\n",
       "      <td>...</td>\n",
       "      <td>99999.99</td>\n",
       "      <td>99999.99</td>\n",
       "      <td>99999.99</td>\n",
       "      <td>99999.99</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>99.99</td>\n",
       "      <td>99.99</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           P1        P2    P4     P5       P6      P10      P11        C1  \\\n",
       "Min -999999.9       0.0 -90.0 -180.0 -12000.0 -12000.0 -12000.0 -999999.9   \n",
       "Max  999999.9  999999.9  90.0  180.0   9000.0   9000.0   9000.0  999999.9   \n",
       "\n",
       "          C4       C5  ...       C27       C28       C29       C30      C33  \\\n",
       "Min      0.0      0.0  ... -99999.99 -99999.99 -99999.99 -99999.99      0.0   \n",
       "Max  19999.9  19999.9  ...  99999.99  99999.99  99999.99  99999.99  99999.0   \n",
       "\n",
       "         C34       C37    C39    C40     C47  \n",
       "Min      0.0       0.0   0.00   0.00     0.0  \n",
       "Max  99999.0  999999.0  99.99  99.99  9999.0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tndf = ndf.transpose()\n",
    "tndf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8f02e-213d-49d9-a8ed-ac5f9f3af38c",
   "metadata": {},
   "source": [
    "## 2.3. String value sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9b73d",
   "metadata": {},
   "source": [
    "    [Description]: Assign the nature of HF data entry extraction metod: borehole/mine or probe sensing. Also provide controlled vocabulary for columns that store string values. By controlled vocabulary it means the permissible options stored as values for a given column. It is possible to store multiple values in the same column for a particular entry. The allowed controlled vocabulary is taken from \"Appendix A. Structure and field definitions of the IHFC Global Heat Flow Database\" in the aforementioned paper. Also revised for the newer release 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2d3dc0f-c273-4f78-9bbd-2788a91cd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87ecc925-fe12-4686-aa3e-a08d10e2749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, P, U = assign_values()\n",
    "sP7 = [\"[Onshore (continental)]\",\"[Onshore (lake, river, etc.)]\",\"[Offshore (continental)]\",\"[Offshore (marine)]\",\"[unspecified]\"];\n",
    "sP9=sC9 = [\"[Yes]\",\"[No]\",\"[Unspecified]\"];\n",
    "sP12 = [\"[Drilling]\",\"[Mining]\",\"[Tunneling]\",\"[GTM]\",\"[Indirect (GTM, CPD, etc.)]\",\"[Probing (onshore/lake, river, etc.)]\",\"[Probing (offshore/ocean)]\",\"[Drilling-Clustering]\",\"[Probing-Clustering]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sP13 = [\"[Hydrocarbon]\",\"[Underground storage]\",\"[Geothermal]\",\"[Groundwater]\",\"[Mapping]\",\"[Research]\",\"[Mining]\",\"[Tunneling]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sC3 = [\"[Interval method]\",\"[Bullard method]\",\"[Boot-strapping method]\",\"[Other numerical computations]\",\"[Other (specify in coments)]\",\"[unspecified]\"];\n",
    "sC11 = [\"[Considered – p]\",\"[Considered – T]\",\"[Considered – pT]\",\"[not considered]\",\"[unspecified]\"];\n",
    "sC12 = [\"[Tilt corrected]\",\"[Drift corrected]\",\"[not corrected]\",\"[Corrected (specify)]\",\"[unspecified]\"];\n",
    "sC13=sC14=sC15=sC16=sC17=sC18=sC19 = [\"[Present and corrected]\",\"[Present and not corrected]\",\"[Present not significant]\",\"[not recognized]\",\"[unspecified]\"];\n",
    "sC20 = [\"[Expedition/Cruise number]\",\"[R/V Ship]\",\"[D/V Platform]\",\"[D/V Glomar Challenger]\",\"[D/V JOIDES Resolution]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sC21 = [\"[Single Steel probe (Bullard)]\",\"[Single Steel probe (Bullard) in-situ TC]\",\"[Violin-Bow probe (Lister)]\",\"[Outrigger probe (Von Herzen) in-situ TC, without corer]\",\"[Outrigger probe (Haenel) in-situ TC, with corer]\",\"[Outrigger probe (Ewing) with corer]\",\"[Outrigger probe (Ewing) without corer]\",\"[Outrigger probe (Lister) with corer]\",\"[Outrigger probe (autonomous) without corer]\",\"[Outrigger probe (autonomous) with corer]\",\"[Submersible probe]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sC31 = [\"[LOGeq]\",\"[LOGpert]\",\"[cLOG]\",\"[DTSeq]\",\"[DTSpert]\",\"[cDTS]\",\"[BHT]\",\"[cBHT]\",\"[HT-FT]\",\"[cHT-FT]\",\"[RTDeq]\",\"[RTDpert]\",\"[cRTD]\",\"[CPD]\",\"[XEN]\",\"[GTM]\",\"[BSR]\",\"[BLK]\",\"[ODTT-PC]\",\"[ODTT-TP]\",\"[SUR]\",\"[GRT]\",\"[EGRT]\",\"[unspecified]\",\"[Other (specify in comments)]\"];\n",
    "sC32 = [\"[LOGeq]\",\"[LOGpert]\",\"[cLOG]\",\"[DTSeq]\",\"[DTSpert]\",\"[cDTS]\",\"[BHT]\",\"[cBHT]\",\"[HT-FT]\",\"[cHT-FT]\",\"[RTDeq]\",\"[RTDpert]\",\"[cRTD]\",\"[CPD]\",\"[XEN]\",\"[GTM]\",\"[BSR]\",\"[BLK]\",\"[ODTT-PC]\",\"[ODTT-TP]\",\"[GRT]\",\"[EGRT]\",\"[unspecified]\",\"[Other (specify in comments)]\"];\n",
    "sC35=sC36 = [\"[Horner plot]\",\"[Cylinder source method]\",\"[Line source explosion method]\",\"[Inverse numerical modelling]\",\"[Other published correction]\",\"[unspecified]\",\"[not corrected]\",\"[AAPG correction]\",\"[Harrison correction]\"];  \n",
    "sC41 = [\"[In-situ probe]\",\"[Core-log integration]\",\"[Core samples]\",\"[Cutting samples]\",\"[Outcrop samples]\",\"[Well-log interpretation]\",\"[Mineral computation]\",\"[Assumed from literature]\",\"[other (specify)]\",\"[unspecified]\"];\n",
    "sC42 = [\"[Actual heat-flow location]\",\"[Other location]\",\"[Literature/unspecified]\",\"[Unspecified]\"];\n",
    "sC43 = [\"[Lab - point source]\",\"[Lab - line source / full space]\",\"[Lab - line source / half space]\",\"[Lab - plane source / full space]\",\"[Lab - plane source / half space]\",\"[Lab - other]\",\"[Probe - pulse technique]\",\"[Well-log - deterministic approach]\",\"[Well-log - empirical equation]\",\"[Estimation - from chlorine content]\",\"[Estimation - from water content/porosity]\",\"[Estimation - from lithology and literature]\",\"[Estimation - from mineral composition]\",\"[unspecified]\"];\n",
    "sC44 = [\"[Saturated measured in-situ]\",\"[Recovered]\",\"[Saturated measured]\",\"[Saturated calculated]\",\"[Dry measured]\",\"[other (specify)]\",\"[unspecified]\"];\n",
    "sC45 = [\"[Unrecorded ambient pT conditions]\",\"[Recorded ambient pT conditions]\",\"[Actual in-situ (pT) conditions]\",\"[Replicated in-situ (p)]\",\"[Replicated in-situ (T)]\",\"[Replicated in-situ (pT)]\",\"[Corrected in-situ (p)]\",\"[Corrected in-situ (T)]\",\"[Corrected in-situ (pT)]\",\"[unspecified]\"];\n",
    "sC46 = [\"[T - Birch and Clark (1940)]\",\"[T - Tikhomirov (1968)]\",\"[T - Kutas & Gordienko (1971)]\",\"[T - Anand et al. (1973)]\",\"[T - Haenel & Zoth (1973)]\",\"[T - Blesch et al. (1983)]\",\"[T - Sekiguchi (1984)]\",\"[T - Chapman et al. (1984)]\",\"[T - Zoth & Haenel (1988)]\",\"[T - Somerton (1992)]\",\"[T - Sass et al. (1992)]\",\"[T - Funnell et al. (1996)]\",\"[T - Kukkonen et al. (1999)]\",\"[T - Seipold (2001)]\",\"[T - Vosteen & Schellschmidt (2003)]\",\"[T - Sun et al. (2017)]\",\"[T - Miranda et al. (2018)]\",\"[T - Ratcliffe (1960)]\",\"[p - Bridgman (1924)]\",\"[p - Sibbitt (1975)]\",\"[p - Kukkonen et al. (1999)]\",\"[p - Seipold (2001)]\",\"[p - Durutürk et al. (2002)]\",\"[p - Demirci et al. (2004)]\",\"[p - Görgülü et al. (2008)]\",\"[p - Fuchs & Förster (2014)]\",\"[pT - Ratcliffe (1960)]\",\"[pT - Buntebarth (1991)]\",\"[pT - Chapman & Furlong (1992)]\",\"[pT - Emirov et al. (1997)]\",\"[pT - Abdulagatov et al. (2006)]\",\"[pT - Emirov & Ramazanova (2007)]\",\"[pT - Abdulagatova et al. (2009)]\",\"[pT - Ramazanova & Emirov (2010)]\",\"[pT - Ramazanova & Emirov (2012)]\",\"[pT - Emirov et al. (2017)]\",\"[pT - Hyndman et al. (1974)]\",\"[Site-specific experimental relationships]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sC48 = [f\"[Random or periodic depth sampling ({number})]\",\"[Characterize formation conductivities]\",\"[Well log interpretation]\",\"[Computation from probe sensing]\",\"[Other]\",\"[unspecified]\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2bdd67",
   "metadata": {},
   "source": [
    "    [Description]: To store the controlled vocabulary in a dataframe structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f91bf08f-26c2-43f9-a829-771fbab2b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_data = {\n",
    "    #'ID': ['P7','P9','P12','P13','C3','C11','C12','C13','C14','C15','C17','C18','C19','C21','C31','C32','C35','C36','C41','C42','C43','C44','C45','C46','C48'],#,C20\n",
    "    'Values': [sP7,sP9,sP12,sP13,sC3,sC11,sC12,sC13,sC14,sC15,sC17,sC18,sC19,sC21,sC31,sC32,sC35,sC36,sC41,sC42,sC43,sC44,sC45,sC46,sC48],#,sC20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8b8d342-c6b6-42a9-bb7f-4d7145355e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.DataFrame(str_data, index=StrC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e996c8c-1809-4b43-8468-309b9b98a422",
   "metadata": {},
   "source": [
    "### 2.3.1. Pivot the DataFrame: Rows become columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c499ef1a-d2ff-47d4-adc6-0618313405ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P7</th>\n",
       "      <th>P9</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>C3</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>...</th>\n",
       "      <th>C32</th>\n",
       "      <th>C35</th>\n",
       "      <th>C36</th>\n",
       "      <th>C41</th>\n",
       "      <th>C42</th>\n",
       "      <th>C43</th>\n",
       "      <th>C44</th>\n",
       "      <th>C45</th>\n",
       "      <th>C46</th>\n",
       "      <th>C48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <td>[[Onshore (continental)], [Onshore (lake, rive...</td>\n",
       "      <td>[[Yes], [No], [Unspecified]]</td>\n",
       "      <td>[[Drilling], [Mining], [Tunneling], [GTM], [In...</td>\n",
       "      <td>[[Hydrocarbon], [Underground storage], [Geothe...</td>\n",
       "      <td>[[Interval method], [Bullard method], [Boot-st...</td>\n",
       "      <td>[[Considered – p], [Considered – T], [Consider...</td>\n",
       "      <td>[[Tilt corrected], [Drift corrected], [not cor...</td>\n",
       "      <td>[[Present and corrected], [Present and not cor...</td>\n",
       "      <td>[[Present and corrected], [Present and not cor...</td>\n",
       "      <td>[[Present and corrected], [Present and not cor...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[LOGeq], [LOGpert], [cLOG], [DTSeq], [DTSpert...</td>\n",
       "      <td>[[Horner plot], [Cylinder source method], [Lin...</td>\n",
       "      <td>[[Horner plot], [Cylinder source method], [Lin...</td>\n",
       "      <td>[[In-situ probe], [Core-log integration], [Cor...</td>\n",
       "      <td>[[Actual heat-flow location], [Other location]...</td>\n",
       "      <td>[[Lab - point source], [Lab - line source / fu...</td>\n",
       "      <td>[[Saturated measured in-situ], [Recovered], [S...</td>\n",
       "      <td>[[Unrecorded ambient pT conditions], [Recorded...</td>\n",
       "      <td>[[T - Birch and Clark (1940)], [T - Tikhomirov...</td>\n",
       "      <td>[[Random or periodic depth sampling (0)], [Cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       P7  \\\n",
       "Values  [[Onshore (continental)], [Onshore (lake, rive...   \n",
       "\n",
       "                                  P9  \\\n",
       "Values  [[Yes], [No], [Unspecified]]   \n",
       "\n",
       "                                                      P12  \\\n",
       "Values  [[Drilling], [Mining], [Tunneling], [GTM], [In...   \n",
       "\n",
       "                                                      P13  \\\n",
       "Values  [[Hydrocarbon], [Underground storage], [Geothe...   \n",
       "\n",
       "                                                       C3  \\\n",
       "Values  [[Interval method], [Bullard method], [Boot-st...   \n",
       "\n",
       "                                                      C11  \\\n",
       "Values  [[Considered – p], [Considered – T], [Consider...   \n",
       "\n",
       "                                                      C12  \\\n",
       "Values  [[Tilt corrected], [Drift corrected], [not cor...   \n",
       "\n",
       "                                                      C13  \\\n",
       "Values  [[Present and corrected], [Present and not cor...   \n",
       "\n",
       "                                                      C14  \\\n",
       "Values  [[Present and corrected], [Present and not cor...   \n",
       "\n",
       "                                                      C15  ...  \\\n",
       "Values  [[Present and corrected], [Present and not cor...  ...   \n",
       "\n",
       "                                                      C32  \\\n",
       "Values  [[LOGeq], [LOGpert], [cLOG], [DTSeq], [DTSpert...   \n",
       "\n",
       "                                                      C35  \\\n",
       "Values  [[Horner plot], [Cylinder source method], [Lin...   \n",
       "\n",
       "                                                      C36  \\\n",
       "Values  [[Horner plot], [Cylinder source method], [Lin...   \n",
       "\n",
       "                                                      C41  \\\n",
       "Values  [[In-situ probe], [Core-log integration], [Cor...   \n",
       "\n",
       "                                                      C42  \\\n",
       "Values  [[Actual heat-flow location], [Other location]...   \n",
       "\n",
       "                                                      C43  \\\n",
       "Values  [[Lab - point source], [Lab - line source / fu...   \n",
       "\n",
       "                                                      C44  \\\n",
       "Values  [[Saturated measured in-situ], [Recovered], [S...   \n",
       "\n",
       "                                                      C45  \\\n",
       "Values  [[Unrecorded ambient pT conditions], [Recorded...   \n",
       "\n",
       "                                                      C46  \\\n",
       "Values  [[T - Birch and Clark (1940)], [T - Tikhomirov...   \n",
       "\n",
       "                                                      C48  \n",
       "Values  [[Random or periodic depth sampling (0)], [Cha...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsdf = sdf.transpose()\n",
    "tsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7defc83-bd46-4558-951d-5f19f87fe999",
   "metadata": {},
   "source": [
    "## 2.4. Case sensitivity issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02c03d",
   "metadata": {},
   "source": [
    "    [Description]: To avoid case-sensitivity issues in the controlled vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b687305e-1f3a-4c61-aec3-60fa1102f906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P7</th>\n",
       "      <th>P9</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>C3</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>...</th>\n",
       "      <th>C32</th>\n",
       "      <th>C35</th>\n",
       "      <th>C36</th>\n",
       "      <th>C41</th>\n",
       "      <th>C42</th>\n",
       "      <th>C43</th>\n",
       "      <th>C44</th>\n",
       "      <th>C45</th>\n",
       "      <th>C46</th>\n",
       "      <th>C48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <td>[[onshore (continental)], [onshore (lake, rive...</td>\n",
       "      <td>[[yes], [no], [unspecified]]</td>\n",
       "      <td>[[drilling], [mining], [tunneling], [gtm], [in...</td>\n",
       "      <td>[[hydrocarbon], [underground storage], [geothe...</td>\n",
       "      <td>[[interval method], [bullard method], [boot-st...</td>\n",
       "      <td>[[considered – p], [considered – t], [consider...</td>\n",
       "      <td>[[tilt corrected], [drift corrected], [not cor...</td>\n",
       "      <td>[[present and corrected], [present and not cor...</td>\n",
       "      <td>[[present and corrected], [present and not cor...</td>\n",
       "      <td>[[present and corrected], [present and not cor...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[logeq], [logpert], [clog], [dtseq], [dtspert...</td>\n",
       "      <td>[[horner plot], [cylinder source method], [lin...</td>\n",
       "      <td>[[horner plot], [cylinder source method], [lin...</td>\n",
       "      <td>[[in-situ probe], [core-log integration], [cor...</td>\n",
       "      <td>[[actual heat-flow location], [other location]...</td>\n",
       "      <td>[[lab - point source], [lab - line source / fu...</td>\n",
       "      <td>[[saturated measured in-situ], [recovered], [s...</td>\n",
       "      <td>[[unrecorded ambient pt conditions], [recorded...</td>\n",
       "      <td>[[t - birch and clark (1940)], [t - tikhomirov...</td>\n",
       "      <td>[[random or periodic depth sampling (0)], [cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       P7  \\\n",
       "Values  [[onshore (continental)], [onshore (lake, rive...   \n",
       "\n",
       "                                  P9  \\\n",
       "Values  [[yes], [no], [unspecified]]   \n",
       "\n",
       "                                                      P12  \\\n",
       "Values  [[drilling], [mining], [tunneling], [gtm], [in...   \n",
       "\n",
       "                                                      P13  \\\n",
       "Values  [[hydrocarbon], [underground storage], [geothe...   \n",
       "\n",
       "                                                       C3  \\\n",
       "Values  [[interval method], [bullard method], [boot-st...   \n",
       "\n",
       "                                                      C11  \\\n",
       "Values  [[considered – p], [considered – t], [consider...   \n",
       "\n",
       "                                                      C12  \\\n",
       "Values  [[tilt corrected], [drift corrected], [not cor...   \n",
       "\n",
       "                                                      C13  \\\n",
       "Values  [[present and corrected], [present and not cor...   \n",
       "\n",
       "                                                      C14  \\\n",
       "Values  [[present and corrected], [present and not cor...   \n",
       "\n",
       "                                                      C15  ...  \\\n",
       "Values  [[present and corrected], [present and not cor...  ...   \n",
       "\n",
       "                                                      C32  \\\n",
       "Values  [[logeq], [logpert], [clog], [dtseq], [dtspert...   \n",
       "\n",
       "                                                      C35  \\\n",
       "Values  [[horner plot], [cylinder source method], [lin...   \n",
       "\n",
       "                                                      C36  \\\n",
       "Values  [[horner plot], [cylinder source method], [lin...   \n",
       "\n",
       "                                                      C41  \\\n",
       "Values  [[in-situ probe], [core-log integration], [cor...   \n",
       "\n",
       "                                                      C42  \\\n",
       "Values  [[actual heat-flow location], [other location]...   \n",
       "\n",
       "                                                      C43  \\\n",
       "Values  [[lab - point source], [lab - line source / fu...   \n",
       "\n",
       "                                                      C44  \\\n",
       "Values  [[saturated measured in-situ], [recovered], [s...   \n",
       "\n",
       "                                                      C45  \\\n",
       "Values  [[unrecorded ambient pt conditions], [recorded...   \n",
       "\n",
       "                                                      C46  \\\n",
       "Values  [[t - birch and clark (1940)], [t - tikhomirov...   \n",
       "\n",
       "                                                      C48  \n",
       "Values  [[random or periodic depth sampling (0)], [cha...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in tsdf.columns:\n",
    "    for id in tsdf.index:\n",
    "        if isinstance(tsdf.loc[id, col], list):\n",
    "            tsdf.loc[id, col] = [str(item).lower() for item in tsdf.loc[id, col]]\n",
    "tsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba2d33-7fb4-4923-a4ae-b21d531144d0",
   "metadata": {},
   "source": [
    "# 3. Data type handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1f913-5016-44ec-ae4c-1167536ff823",
   "metadata": {},
   "source": [
    "## 3.1. Assigning data types to specific columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5608178",
   "metadata": {},
   "source": [
    "    [Description]: Convert all the columns to string data type. To resolve multiple values in a categorical field for an entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7355f55e-e4fd-4663-bb6a-704836912581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(df):\n",
    "    df[NumC] = df[NumC].astype(str)\n",
    "    df[StrC] = df[StrC].astype(str)\n",
    "    df[DateC] = df[DateC].astype(str)   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55399ce3-288e-4dc0-b7e5-bd9b239010af",
   "metadata": {},
   "source": [
    "# 4. Converting string values to lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945796f",
   "metadata": {},
   "source": [
    "    [Description]: To resolve case-sensitivity in the provided Heatflow database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fee3591-46d2-4279-92af-e506d90a84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLower(df):\n",
    "    for col in tsdf.columns:\n",
    "        for id in df.index:\n",
    "            df.loc[id, col] = (df.loc[id, col]).lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f877e-c6eb-41fd-bf08-11da8e416595",
   "metadata": {},
   "source": [
    "# 5. Check relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f4e3a",
   "metadata": {},
   "source": [
    "## 5.1. Obligation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7161171",
   "metadata": {},
   "source": [
    "    [Description]: Check for mandatory fields indicated by 'Obligation' label in HF database. And store information about the nature of data, whether its borehole or probe sensing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8570b129-5df2-4764-b1fc-cc42f33e8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obligation(df):\n",
    "    m_dict = {}\n",
    "    domain = {}\n",
    "    for c in df:\n",
    "        m_dict[c] = df.loc[0, c]\n",
    "        domain[c] = df.loc[1, c]\n",
    "    return m_dict, domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310b279",
   "metadata": {},
   "source": [
    "## 5.2.  Structure relevance for the current release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc9c47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "    if csv_files:\n",
    "        first_csv_file_path = os.path.join(folder_path, csv_files[0])\n",
    "        df = pd.read_csv(first_csv_file_path)\n",
    "        m_dict, domain = obligation(df)\n",
    "    else:\n",
    "        print(\"No CSV files found in the directory. Please run 'convert2UTF8csv(folder_path)' function\")\n",
    "    return m_dict, domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea4dbc-21cc-47fb-b4d3-56156f4e0bfc",
   "metadata": {},
   "source": [
    "# 6. Vocabulary check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9c7d3",
   "metadata": {},
   "source": [
    "    [Description]: Complete check of vocabulary separately for numeric, string and date type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67c3e7b8-9945-4c9b-a8e6-3a663868f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabcheck(df,m_dict,domain):\n",
    "    error_df = pd.DataFrame()\n",
    "    error_msg =pd.DataFrame()\n",
    "    error_msg_counter = 0\n",
    "\n",
    "    for id in df.index:\n",
    "        error_df.loc[id,'A'] = None\n",
    "        error_df['A'] = error_df['A'].astype(\"string\")\n",
    "\n",
    "        P12_split = (df.loc[id, 'P12']).split(';')\n",
    "        \n",
    "        if any(value in ['[other (specify in comments)]', '[unspecified]'] for value in P12_split):\n",
    "            error_string = \" P12:Quality Check is not possible!,\"\n",
    "        elif any(value == 'nan' for value in P12_split):\n",
    "            error_string = \" P12:Mandatory entry is empty; Quality Check is not possible!,\"\n",
    "        elif any(value in P for value in P12_split) and any(value in B for value in P12_split):\n",
    "            error_string = \" P12:Quality Check is not possible!,\"\n",
    "        else:\n",
    "            error_string = \"\"\n",
    "\n",
    "        error_df.loc[id,'A'] = error_string\n",
    "        \n",
    "\n",
    "    for c in NumC:\n",
    "        min_value = tndf.loc['Min', c]\n",
    "        max_value = tndf.loc['Max', c]\n",
    "        \n",
    "        for id in df.index:\n",
    "            error_df.loc[id,c] = None\n",
    "            error_df[c] = error_df[c].astype(\"string\")\n",
    "            dfvalue = df.loc[id,c]\n",
    "\n",
    "            P12_split = (df.loc[id, 'P12']).split(';')\n",
    "            if any(value in U for value in P12_split):\n",
    "                P12 = \"\"\n",
    "            elif any(value in P for value in P12_split) and any(value in B for value in P12_split):\n",
    "                P12 = \"\"\n",
    "            else:\n",
    "                P12 = P12_split[0] if P12_split else df.loc[id, 'P12']\n",
    "\n",
    "            while True:\n",
    "                dfvalue = dfvalue.split(';')\n",
    "                \n",
    "                for dfvalue in dfvalue:\n",
    "                    try:\n",
    "                        r = dfvalue.strip()\n",
    "                        if float(r) or r=='0':\n",
    "                            r = safe_float_conversion(r)\n",
    "    \n",
    "                            if  min_value <= r <= max_value:\n",
    "                                error_string = \"\"\n",
    "                                \n",
    "                            elif math.isnan(r):\n",
    "                                if (m_dict[c] == 'M') and (df.loc[id, c]) == 'nan':\n",
    "                                    if ('B' in domain[c] and (P12 in B)):\n",
    "                                        error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                    elif ('S' in domain[c] and (P12 in P)):                                        \n",
    "                                        if (c == 'C4') and (df.loc[id, 'P6']) != 'nan':\n",
    "                                            error_string = \"\"\n",
    "                                        else:\n",
    "                                            error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                            \n",
    "                                    elif ((('B'or'S') in domain[c]) and (P12 in U)):\n",
    "                                        error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                    else:\n",
    "                                        error_string = \"\"\n",
    "                                elif m_dict[c] == 'M':\n",
    "                                    if P12 in B:\n",
    "                                        if (c == 'C5') and (df.loc[id, 'C6'] is None):\n",
    "                                            error_string = f\" {c}:mandatory field!,\"   \n",
    "                                        else:\n",
    "                                            error_string = \"\"\n",
    "                                    elif P12 in P:\n",
    "                                        if (c == 'C6') and (df.loc[id, 'C5'] is None):\n",
    "                                            error_string = f\" {c}:mandatory field!,\"\n",
    "                                        elif (c == 'C23') and ((df.loc[id, 'C31'] or df.loc[id, 'C32']) is None):\n",
    "                                            error_string = f\" {c}:mandatory field!,\"\n",
    "                                        else:\n",
    "                                            error_string = \"\"\n",
    "                                else:\n",
    "                                    error_string = \"\"\n",
    "                            else:\n",
    "                                error_string = f\" {c}:range violated,\" ###                                                                      \n",
    "                        else:\n",
    "                            error_string = f\" {c}:range violated,\"\n",
    "                    except ValueError:\n",
    "                            error_string = f\" {c}:invalid format,\" \n",
    "                        \n",
    "                    error_df.loc[id,c] = error_string\n",
    "                    if error_string != \"\":\n",
    "                        error_msg_counter= error_msg_counter+1\n",
    "\n",
    "                if ';' not in dfvalue:\n",
    "                    break\n",
    "                else:\n",
    "                    dfvalue = dfvalue[-1]\n",
    "                         \n",
    "    for c in StrC:\n",
    "        string_values = tsdf.loc['Values', c]\n",
    "\n",
    "        for id in df.index:\n",
    "            error_df.loc[id,c] = None\n",
    "            error_df[c] = error_df[c].astype(\"string\")\n",
    "            dfvalue = df.loc[id,c]\n",
    "\n",
    "            P12_split = (df.loc[id, 'P12']).split(';')\n",
    "\n",
    "            if any(value in U for value in P12_split):\n",
    "                P12 = \"\"\n",
    "            elif any(value in P for value in P12_split) and any(value in B for value in P12_split):\n",
    "                P12 = \"\"\n",
    "            else:\n",
    "                P12 = P12_split[0] if P12_split else df.loc[id, 'P12']\n",
    "\n",
    "            while True:\n",
    "                dfvalue = dfvalue.split(';')\n",
    "\n",
    "                for dfvalue in dfvalue:\n",
    "                    dfvalue = dfvalue.strip()\n",
    "                    # new modifications\n",
    "                    if (c == 'C48') and (dfvalue == \"[random or periodic depth sampling (number)]\"):\n",
    "                        error_string = \"\"\n",
    "                    elif (c == 'C48') and (dfvalue.startswith(\"[random or periodic depth sampling (\")):\n",
    "                        start_idx = dfvalue.find('(')\n",
    "                        end_idx = dfvalue.find(')')\n",
    "                        number_str = dfvalue[start_idx + 1:end_idx]\n",
    "                        \n",
    "                        try:\n",
    "                            number = int(number_str)\n",
    "                            string_values[0] = f\"[random or periodic depth sampling ({number})]\"\n",
    "                                                                                 \n",
    "                            if dfvalue in string_values:\n",
    "                                error_string = \"\"\n",
    "                            else:\n",
    "                                error_string = f\" {c}:vocabulary warning,\"\n",
    "                            \n",
    "                        except ValueError: \n",
    "                            # new modifications\n",
    "                            error_string = f\" {c}:Enter a number,\"\n",
    "                            \n",
    "                    elif (c == 'C43') and ('[egrt]' in (df.loc[id, 'C31'] or df.loc[id, 'C32'])):\n",
    "                        if dfvalue == \"[probe - pulse technique]\":\n",
    "                            error_string = ''\n",
    "                        else:\n",
    "                            error_string = f\" {c}:Please check TC method!,\"\n",
    "\n",
    "                    elif dfvalue in string_values:\n",
    "                        error_string = \"\"\n",
    "        \n",
    "                    elif dfvalue == 'nan':\n",
    "                        if m_dict[c] == 'M':\n",
    "                            if (c == 'C31' or 'C32') and (df.loc[id, 'C23'] is None):\n",
    "                                error_string = f\" {c}:mandatory field!,\"\n",
    "                            else:\n",
    "                                if ('B' in domain[c] and (P12 in B)):\n",
    "                                    error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                elif ('S' in domain[c] and (P12 in P)):\n",
    "                                    error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                elif ((('B'or'S') in domain[c]) and (P12 in U)):\n",
    "                                    error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                else:\n",
    "                                    error_string = \"\" #pass\n",
    "                        else:\n",
    "                            error_string = \"\"       \n",
    "                    else:\n",
    "                        error_string = f\" {c}:vocabulary warning,\"\n",
    "        \n",
    "                    error_df.loc[id,c] = error_string\n",
    "                    if error_string != \"\":\n",
    "                        error_msg_counter= error_msg_counter+1\n",
    "                        \n",
    "                if ';' not in dfvalue:\n",
    "                    break\n",
    "                else:\n",
    "                    dfvalue = dfvalue[-1]\n",
    "    \n",
    "    # Compare the input date with January 1900\n",
    "    jan_1900 = datetime(1900, 1, 1)\n",
    "    for id in df.index:\n",
    "        error_df.loc[id,'C38'] = None\n",
    "        error_df['C38'] = error_df['C38'].astype(\"string\")\n",
    "        dfvalue = (df.loc[id,'C38']).lower()\n",
    "\n",
    "        P12_split = (df.loc[id, 'P12']).split(';')\n",
    "\n",
    "        if any(value in U for value in P12_split):\n",
    "            P12 = \"\"\n",
    "        elif any(value in P for value in P12_split) and any(value in B for value in P12_split):\n",
    "            P12 = \"\"\n",
    "        else:\n",
    "            P12 = P12_split[0] if P12_split else df.loc[id, 'P12']\n",
    "\n",
    "        while True:\n",
    "                dfvalue = dfvalue.split(';')\n",
    "                    \n",
    "                for dfvalue in dfvalue:\n",
    "                    dfvalue = dfvalue.strip()\n",
    "                    \n",
    "                    if dfvalue == '[unspecified]':\n",
    "                        error_string = \"\"\n",
    "                    elif df.loc[id, 'C38'] == 'nan':\n",
    "                        if ('B' in domain[c] and (P12 in B)):\n",
    "                            error_string = \" C38:Mandatory entry is empty!,\"\n",
    "                        elif ('S' in domain[c] and (P12 in P)):\n",
    "                            error_string = \" C38:Mandatory entry is empty!,\"\n",
    "                        elif ((('B'or'S') in domain[c]) and (P12 in U)):\n",
    "                            error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                        else:\n",
    "                            error_string = \"\" #pass\n",
    "                    else:                        \n",
    "                        try:\n",
    "                            if dfvalue[-2:] == \"99\":\n",
    "                                year = int(dfvalue[:4])\n",
    "                                input_date = datetime(year, 1, 1)\n",
    "                            else:\n",
    "                                input_date = datetime.strptime(dfvalue, '%Y-%m')\n",
    "                            \n",
    "                            if input_date.month == 1 and input_date.year >= jan_1900.year:\n",
    "                                error_string = \"\"\n",
    "                            elif input_date >= jan_1900:\n",
    "                                error_string = \"\"\n",
    "                            else:\n",
    "                                error_string = \" C38:range violated\"\n",
    "                        except ValueError:\n",
    "                            error_string = f\" C38:invalid format,\"\n",
    "                    if error_string != \"\":\n",
    "                            error_msg_counter= error_msg_counter+1\n",
    "            \n",
    "                    error_df.loc[id,'C38'] = error_string\n",
    "                error_df = error_df.astype(\"string\")\n",
    "                    \n",
    "                if ';' not in dfvalue:\n",
    "                    break\n",
    "                else:\n",
    "                    dfvalue = dfvalue[-1]\n",
    "        \n",
    "    result = error_df.apply(lambda x: ''.join(x), axis=1)\n",
    "    result = result.astype(\"string\")\n",
    "    \n",
    "    error_msg['Error'] = result\n",
    "\n",
    "    return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3539b-7a4e-4cbf-b859-f7620bdcc056",
   "metadata": {},
   "source": [
    "# 7. Final check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ff9df-f214-4483-ae0f-45cb62d7533c",
   "metadata": {},
   "source": [
    "## 7.1 Sort error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9a4eb7d-7a54-4500-afa4-5a5ebd803360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_errors(error_str):\n",
    "    #errors = error_str.split(', ')\n",
    "    errors = re.split(r',\\s*', error_str.strip())\n",
    "    \n",
    "    p_errors = [e for e in errors if e.startswith('P')]\n",
    "    c_errors = [e for e in errors if e.startswith('C')]\n",
    "    \n",
    "    p_errors_sorted = sorted(p_errors, key=lambda x: int(x[1:x.index(':')]))\n",
    "    c_errors_sorted = sorted(c_errors, key=lambda x: int(x[1:x.index(':')]))\n",
    "    \n",
    "    sorted_errors = p_errors_sorted + c_errors_sorted\n",
    "    \n",
    "    sorted_errors_str = ', '.join(sorted_errors)\n",
    "    \n",
    "    cleaned_errors_str = re.sub(r',\\s*,\\s*', ', ', sorted_errors_str)\n",
    "\n",
    "    if cleaned_errors_str.endswith(','):\n",
    "        cleaned_errors_str = cleaned_errors_str[:-1]\n",
    "    \n",
    "    return cleaned_errors_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cf941-3ec2-49e0-958e-3a3460d7469f",
   "metadata": {},
   "source": [
    "## 7.2 Complete check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21daba",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform vocabulary checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88245463-65de-4a0c-a492-c56a44dedd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complete_check(df):\n",
    "    m_dict, domain = obligation(df)\n",
    "    result = vocabcheck(toLower(change_type(remove_head(df))), m_dict, domain)\n",
    "    result['Error'] = result['Error'].apply(reorder_errors)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e790ae",
   "metadata": {},
   "source": [
    "# 8 Attach to original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3e4a8",
   "metadata": {},
   "source": [
    "    [Description]: Attaching the combined results column to the original database with correct indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42189f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attachOG(og):\n",
    "    result = Complete_check(og)\n",
    "    if og.at[0, 'ID'] == 'Obligation':\n",
    "\n",
    "        result.index = result.index + 6\n",
    "    elif og.at[0, 'ID'] == 'Short Name':\n",
    "\n",
    "        result.index = result.index + 1\n",
    "    \n",
    "    og = pd.merge(og, result[['Error']], left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return og"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139fe9d",
   "metadata": {},
   "source": [
    "# 9. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5407c5",
   "metadata": {},
   "source": [
    "## 9.1 Results of all files in a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f8787",
   "metadata": {},
   "source": [
    "    [Description]: To generate results for all the Heatflow database in a folder stored in .csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98a5a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_result(folder_path):\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))    \n",
    "\n",
    "    for csv_file_path in csv_files:\n",
    "\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        df_result = attachOG(df)\n",
    "\n",
    "        if df_result['Error'].eq('').all():\n",
    "            print(\"There is no error. Data is ready for Quality Check!\")\n",
    "        else:\n",
    "            output_excel_file = os.path.splitext(csv_file_path)[0] + '_vocab_check.xlsx'        \n",
    "            df_result.to_excel(output_excel_file, index=False)\n",
    "            print(f\"Result exported: {output_excel_file}\")\n",
    "\n",
    "    for csv_file_path in csv_files:\n",
    "        os.remove(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e6928",
   "metadata": {},
   "source": [
    "# 10. hfqa_tool function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb0462",
   "metadata": {},
   "source": [
    "     [Description]: To check the vocabulary for all the HF dataframe files in a folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7aac88",
   "metadata": {},
   "source": [
    "     [Desclaimer]: When a new data release occurs and the relevancy (indicated by 'Obligation') of a column in the HF data structure is updated, ensure that you place the data structure files with the updated column relevancy into separate folders before running the code!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a961ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocabulary():\n",
    "    folder_path = input(\"Please enter the file directory for vocabulary check: \")\n",
    "    readable(folder_path)\n",
    "    folder_result(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "200f4aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result exported: Z:\\Tempfeld\\WG\\PROJEKTE\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\trial_papers\\Multivalue\\DellaVedova_etal._1992_vocab_check.xlsx\n",
      "Result exported: Z:\\Tempfeld\\WG\\PROJEKTE\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\trial_papers\\Multivalue\\Feng_etal._2019_vocab_check.xlsx\n",
      "Result exported: Z:\\Tempfeld\\WG\\PROJEKTE\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\trial_papers\\Multivalue\\Kanyuan_etal._1994_vocab_check.xlsx\n",
      "Result exported: Z:\\Tempfeld\\WG\\PROJEKTE\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\trial_papers\\Multivalue\\Mizutani_Yokokura_1982_vocab_check.xlsx\n",
      "Result exported: Z:\\Tempfeld\\WG\\PROJEKTE\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\trial_papers\\Multivalue\\Skinner_1985_vocab_check.xlsx\n",
      "CPU times: total: 3.7 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "check_vocabulary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Heatflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
